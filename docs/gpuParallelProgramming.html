<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
    <meta charset="utf-8">

    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Concurnas: GPU/Parallel programming</title>

    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="../css/bootstrap-toc.min.css">
    <link rel="stylesheet" type="text/css" href="../css/conc-nav.css"/>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="../css/doctheme.css" />
    <link rel="stylesheet" type="text/css" href="../css/docpygments.css" />
    <link rel="stylesheet" type="text/css" href="../css/prism-theme.css"/>
    <link rel="shortcut icon" type="image/x-icon" href="../favicon.ico"/>

    <!-- Google -->
    <meta name="description" content="The Concurnas Language Reference chapter covering: GPU/Parallel programming"/>

    <!-- OpenGraph -->
    <meta property="og:title" content="Concurnas: GPU/Parallel programming"/>
    <meta property="og:type" content="article"/>
    <meta property="og:image" content="http://www.concurnas.com/img/logo.png"/>
    <meta property="og:description" content="The Concurnas Language Reference chapter covering: GPU/Parallel programming"/>
    <meta property="og:site_name" content="The Concurnas Programming Language"/>
    <meta property="og:url" content="http://concurnas.com/docs/gpuParallelProgramming.html" />


  <script>
    var shiftWindow = function() { scrollBy(0, -65) };
    window.addEventListener("hashchange", shiftWindow);
    function load() { if (window.location.hash) shiftWindow(); }
  </script>

</head>

<body class="wy-body-for-nav" onload="load()">

<nav class="section nav dark navbar fixed-top navbar-expand-xl navbar-dark topnav">
    <a class="navbar-brand" href="../index.html"><img alt="Concurnas" height="21" onerror="this.src=&#039;../img/logo-on-dark.png&#039;" src="../img/logo-on-dark.svg" width="107" /></a>

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse " id="navbarSupportedContent">
        <ul class="navbar-nav mr-auto">

            <li class="nav-item"><a class="nav-link" href="../index.html" class="">Home</a></li>
            <li class="nav-item"><a class="nav-link" href="../download.html">Download</a></li>

            <li class="nav-item dropdown">
                <a class="nav-link dropdown-toggle" href="../learnmore/overview.html" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Learn More</a>
                <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                    <a class="dropdown-item" href="../learnmore/overview.html">Language Overview</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="../learnmore/concVsJava1.html">Concurnas vs Java</a>
                    <a class="dropdown-item" href="../learnmore/concVsJava1.html">&bull; part 1</a>
                    <a class="dropdown-item" href="../learnmore/concVsJava2.html">&bull; part 2</a>
                    <a class="dropdown-item" href="../learnmore/concVsJava3.html">&bull; part 3</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="../learnmore/usecases.html">Use cases</a>
                    <a class="dropdown-item" href="../learnmore/finance.html">&bull; Finance</a>
                    <a class="dropdown-item" href="../learnmore/sciComp.html">&bull; Scientific Computing</a>
                    <a class="dropdown-item" href="../learnmore/startups.html">&bull; Startups</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="../learnmore/mailinglist.html">Mailing List</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="../learnmore/contribute.html">Contribute</a>
                </div>
            </li>

            <li class="nav-item dropdown">
                <a class="nav-link dropdown-toggle" href="../docs/manual.html" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Documentation</a>
                <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                    <a class="dropdown-item" href="../docs/manual.html">Reference Manual</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="../video/index.html">Video</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="../docs/communitySupport.html">Community Support</a>
                </div>
            </li>

            <li class=" divider"></li>

            <li class="nav-item dropdown">
                <a class="nav-link dropdown-toggle" href="../news.html" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">News</a>
                <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                    <a class="dropdown-item" href="../news.html">Latest News</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="../newsletters/march2020.html">Newsletters</a>
                </div>
            </li>

            <li class=" divider"></li>
            <li class="nav-item"><a class="nav-link" href="../video/index.html" class="">Video</a></li>
            <li class=" divider"></li>

            <li class="nav-item dropdown">
                <a class="nav-link dropdown-toggle" href="../concurnasltd/introduction.html" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Concurnas Ltd</a>
                <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                    <a class="dropdown-item" href="../concurnasltd/introduction.html">Concurnas Ltd</a>
                    <a class="dropdown-item" href="../concurnasltd/leadership.html">Leadership</a>
                    <a class="dropdown-item" href="../concurnasltd/careers.html">Careers</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="../concurnasltd/open-source.html">Open Source</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="../concurnasltd/support.html">Support</a>
                    <a class="dropdown-item" href="../concurnasltd/consulting.html">Consulting</a>
                    <a class="dropdown-item" href="../concurnasltd/sponsorship.html">Sponsorship</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="../concurnasltd/contact.html">Contact</a>
                </div>
            </li>
            <li class="nav-item"><a class="nav-link" href="../concurnasltd/support.html" class="">Support</a></li>
            <li class="nav-item"><a class="nav-link" href="../concurnasltd/consulting.html" class="">Consulting</a></li>
            <li class=" divider"></li>

            <li class="nav-item dropdown">
                <a class="nav-link dropdown-toggle" href="../concurnasltd/contact.html" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Connect</a>
                <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                    <a class="dropdown-item" href="https://github.com/Concurnas/Concurnas"><i class="fab fa-github fa-fw"></i> GitHub</a>
                    <a class="dropdown-item" href="https://github.com/Concurnas/Concurnas/issues"><i class="fab fa-github fa-fw"></i> Bug reports</a>
                    <a class="dropdown-item" href="https://stackoverflow.com/questions/tagged/concurnas"><i class="fab fa-stack-overflow fa-fw"></i> Stack Overflow</a>
                    <a class="dropdown-item" href="https://discord.gg/jFHfsqR"><i class="fab fa-discord fa-fw"></i> Discord</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="https://www.reddit.com/r/Concurnas/"><i class="fab fa-reddit fa-fw"></i> Reddit</a>
                    <a class="dropdown-item" href="https://www.facebook.com/concurnas/"><i class="fab fa-facebook fa-fw"></i> Facebook</a>
                    <a class="dropdown-item" href="https://www.linkedin.com/company/concurnas/"><i class="fab fa-linkedin fa-fw"></i> Linkedin</a>
                    <a class="dropdown-item" href="https://twitter.com/concurnas"><i class="fab fa-twitter fa-fw"></i> Twitter</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="https://vimeo.com/concurnas"><i class="fab fa-vimeo fa-fw"></i> Vimeo</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="../concurnasltd/contact.html">Contact</a>
                </div>
            </li>

        </ul>

    </div>

</nav> 

    <div class="wy-grid-for-nav">


        <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">




            <div class="wy-nav-content">

                <div class="rst-content">

                    <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
                        <div itemprop="articleBody">

                            <div class="section" id="paragraph-level-markup">
<div class="section">
<h1 id="gpu/parallel-programming">GPU/Parallel programming<a class="headerlink" href="#gpu/parallel-programming" title="Permalink to this headline" id="gpu/parallel-programming">?</a></h1>
<div class="contents topic" id="table-of-contents">
 <p class="topic-title first">Table of Contents</p>
 <ul class="simple">
  <li><a class="reference internal" href="#gpu/parallel-programming">GPU/Parallel programming</a>
   <ul>
   <li><a class="reference internal" href="#prerequisites">Prerequisites</a>   <ul>
   <li><a class="reference internal" href="#code-portability">Code Portability</a>   </li>
   </ul>
   </li>
   <li><a class="reference internal" href="#events">Events</a>   </li>
   <li><a class="reference internal" href="#data-transference">Data transference</a>   <ul>
   <li><a class="reference internal" href="#buffers">Buffers</a>   </li>
   <li><a class="reference internal" href="#writing,-reading-data">Writing, Reading Data</a>   </li>
   <li><a class="reference internal" href="#copying-data">Copying Data</a>   </li>
   <li><a class="reference internal" href="#writing,-reading-and-copying-subregions">Writing, Reading and Copying subregions</a>   </li>
   </ul>
   </li>
   <li><a class="reference internal" href="#sizeof2">sizeof</a>   </li>
   <li><a class="reference internal" href="#kernels-and-functions">Kernels and functions</a>   <ul>
   <li><a class="reference internal" href="#work-items">Work Items</a>   </li>
   <li><a class="reference internal" href="#kernel-dimensions">Kernel dimensions</a>   </li>
   <li><a class="reference internal" href="#kernel-arguments">Kernel arguments</a>   </li>
   <li><a class="reference internal" href="#calling-functions-from-kernels">Calling functions from kernels</a>   </li>
   <li><a class="reference internal" href="#stub-functions">Stub functions</a>   </li>
   <li><a class="reference internal" href="#recursion1">Recursion</a>   </li>
   <li><a class="reference internal" href="#function-overloading">Function overloading</a>   </li>
   <li><a class="reference internal" href="#kernel-and-function-variables">Kernel and function variables</a>   </li>
   <li><a class="reference internal" href="#kernel/function-restrictions">Kernel/function restrictions</a>   </li>
   <li><a class="reference internal" href="#invoking-kernels">Invoking kernels</a>   </li>
   </ul>
   </li>
   <li><a class="reference internal" href="#profiling">Profiling</a>   </li>
   <li><a class="reference internal" href="#pointers">Pointers</a>   <ul>
   <li><a class="reference internal" href="#pointers-and-arrays">Pointers and Arrays</a>   </li>
   <li><a class="reference internal" href="#array-of-pointers">Array of pointers</a>   </li>
   </ul>
   </li>
   <li><a class="reference internal" href="#local-memory">Local memory</a>   <ul>
   <li><a class="reference internal" href="#local-buffers">Local Buffers</a>   </li>
   <li><a class="reference internal" href="#barriers">Barriers</a>   </li>
   <li><a class="reference internal" href="#local-work-size">Local Work Size</a>   </li>
   <li><a class="reference internal" href="#example-kernel-using-local-parameters">Example Kernel using local Parameters</a>   </li>
   <li><a class="reference internal" href="#example-kernel-using-local-variables">Example Kernel using local Variables</a>   </li>
   </ul>
   </li>
   <li><a class="reference internal" href="#explicit-memory-management">Explicit memory management</a>   </li>
   <li><a class="reference internal" href="#finishing">Finishing</a>   </li>
   <li><a class="reference internal" href="#double-precision">Double Precision</a>   </li>
   <li><a class="reference internal" href="#concurrent-use-of-gpus">Concurrent use of GPUs</a>   </li>
   <li><a class="reference internal" href="#notes">Notes</a>   </li>
   </ul>
  </li>
 </ul>
</div>
<blockquote><p>Time to wake up</p>
<p>Refill new destiny</p>
<p>Through this</p><footer>- The Whitest of Lies from Prototype by NeuroWulf (2005)</footer></blockquote>
<p>Concurnas has built in first class citizen support for programming GPUs and associated parallel computing constructs. GPUs are massively data parallel computation devices which are perfect for solving SIMD (single instruction, multiple data) and normally CPU bound problems. Compared to a single CPU core algorithm implementation, in solving a computation bound problem, it is common to be able to obtain up to a 100x speed improvement (two orders of magnitude) when using a GPU! And this is with a far reduced energy and hardware cost per gigaflop relative to CPU based computation. All modern graphics cards have a built in GPU and there exist dedicated GPU computation devices. In fact some of the world's leading supercomputers achieve their parallelization through the use of dedicated GPU hardware. With Concurnas this power is your as well.</p>
<p>Support is provided via interfacing to OpenCL - an excellent multiplatform API for GPU computing. OpenCL is supported by the big three GPU manufacturers - NVidia, AMD and Intel. However, even with conventional, raw OpenCL (or any GPU computing platform for that matter) one must write a lot of boilerplate code and perform a lot of non work related management in order to get computation done on the GPU. A such, authoring large applications can be an intimidating prospect. With Concurnas you will see that this boilerplate code is minimized, allowing the developer to focus on solving real business problems. Additionally there is no need to learn (and have to paradigm shift into) C or C++ which is the native language used by OpenCL for expressing parallel functions on the GPU as functions marked as parallel in Concurnas are automatically translated into a form understandable by the GPU/OpenCL. GPU computing is for everyone.</p>
<p>The helper classes associated with GPU computing are a part of the core Concurnas API. This can be found here.</p>
<p>What follows is a practical guide covering the key components of GPU computing with Concurnas.</p>
<div class="section">
<div class="section">
<h2 id="prerequisites">Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline" id="prerequisites">?</a></h2>
<p>In order to make use of the Concurnas GPU parallel programming solution the following prerequisites must be satisfied:</p>
<ul class="simple">
  <li><p>Access to at least one OpenCL ready GPU on one's computation machine (almost all modern graphics cards support this)</p>
</li>
  <li><p>OpenCL compatible driver compliant to at least version 1.2 of OpenCL for said GPU(s). Consult your graphics card manufacturer for support for details on this.</p>
</li>
</ul class="simple">
<p>Concurnas enables parallel computation on GPUs, FPGA accelerators and conventional CPUs. Although OpenCL natively supports execution on the CPU, we do not encourage use of this in Concurnas for solving anything but strictly data based parallel problems.</p>
<p>To see if one has access to any GPU devices one can use the <code class="lang-conc">getGPUDevices</code> command as part of the GPU API:</p>
<pre><code class="lang-conc">from com.concurnas.lang import gpus

gps = gpus.GPU()
gpudevices = gps.getGPUDevices()
firstDevice = gpudevices[0]
deviceDetails = firstDevice.toString()

//deviceDetails == NVIDIA CUDA: [GeForce GTX 570]
</code></pre>
<p>Note that <code class="lang-conc">com.concurnas.lang.gpus</code> is an auto import, therefore it is not necessary to explicitly import <code class="lang-conc">gpus</code>, and we shall cease to do so in the subsequent examples.</p>
<p>Note that a machine may have more than one GPU (in fact this is common with machines having integrated graphics processors either on the motherboard or CPU - often being Intel HD graphics). But, if no GPU devices are available it can be useful for some problems to 'fall back' on to CPU based computation. The interface/compute model is the same and one will still obtain the advantages of the SIMD instruction set whilst processing on the CPU which is quicker for data parallel problems than the alternatives provided in Concurnas (which are more oriented towards task parallelism). One can select the CPUs available on one's machine via the <code class="lang-conc">getCPUDevices</code>, example:</p>
<pre><code class="lang-conc">def chooseGPUDeviceGroup(gpuDev gpus.DeviceGroup[], cpuDev gpus.DeviceGroup[]) gpus.Device {
  if(not gpuDev){//no gpu return first cpu
    cpuDev[0]
  }else{
    for(grp in gpuDev){
      if('Intel' not in grp.platformName){
        return grp
      }
    }
    gpuDev[0]//intel device then...
  }
}

gps = gpus.GPU()
deviceGrp gpus.DeviceGroup= chooseGPUDeviceGroup(gps.getGPUDevices(), gps.getCPUDevices())
device gpus.Device = deviceGrp.devices[0]
</code></pre>
<p>In the above example we choose the first non Intel based GPU, failing that we fall back to returning the first CPU device available.</p>
<p>We can examine the capabilities of both the device group and the individual devices of which it composes by calling the get methods of interest on the objects. Here we just look at an example summary for both:</p>
<p>For the DeviceGroup:</p>
<pre><code class="lang-conc">gps = gpus.GPU()
deviceGrp = gps.getGPUDevices()[0]

summary = deviceGrp getSummary()
/*
summary =&gt; 
Device Group:   NVIDIA CUDA: [GeForce GTX 570]
OpenCL Version: OpenCL 1.2 CUDA 9.1.84
Vendor:         NVIDIA Corporation
Extensions:     cl_khr_global_int32_base_atomics ...
*/
</code></pre>
<p>For an individual device:</p>
<pre><code class="lang-conc">gps = gpus.GPU()
deviceGrp = gps.getGPUDevices()[0]
device = deviceGrp.devices[0]

summary = device getSummary
/*
summary =&gt; 
Device:                     GeForce GTX 570
Vendor:                     NVIDIA Corporation
Address Bits:               64
Available:                  true
Compiler Available:         true
Little Endian:              true
Error Correction Support:   false
Global Memory Size:         1.3 GB
Local Memory Size:          48.0 kB
Compute Units:              15
Clock Frequency:            1560
Constant Args:              9
Constant Buffer Size:       64.0 kB
Max Memory Allocation Size: 320.0 MB
Param Size:                 4352
Max Work Group Size:        1024
Max Work Item Sizes:        [1024 1024 64]
Base Address Align:         4096
Data Type Align Size:       128
Profiling Timer Resolution: 1000
Type:                       CL_DEVICE_TYPE_GPU
Vendor ID:                  4318
OpenCL Version:             OpenCL 1.1 CUDA
Driver Version:             391.35
Extensions:                 cl_khr_global_int32_base_atomics...
*/
</code></pre>
<div class="section">
<div class="section">
<h3 id="code-portability">Code Portability<a class="headerlink" href="#code-portability" title="Permalink to this headline" id="code-portability">?</a></h3>
<p>When working with ordinary Concurnas code operating on general purpose CPUs, the computing environment and how one codes for it is assumed to be highly homogeneous. I.e. it's rare that one need to consider the clocks speed, amount of L1/L2 cache one's cpu has, or even the amount of RAM available for operation, generally speaking these things are automatically optimized/can be assumed as being adequate for ones software to run. Additionally, one does not need to adapt ones code to optimize for different CPU architectures/RAM configuration etc, this is something which the compiler/virtual machine takes care of.</p>
<p>However, when one is working with GPUs, although there are many optimizations in place as part of the compilation process (e.g. optimal opcode generation, register allocation etc) the decisions required in order to optimally solve a problem on a GPU are generally left for the developer to optimize for, and if one is not diligent, one can write code which is not very portable/optimal for the range of GPUs one's code is run on. There are some properties referenced above which are of particular interest insofar as code portability is concerned, which generally need to be factored in in ones software:</p>
<ul class="simple">
  <li><p><b>Address Bits</b> - This indicates whether the GPU is operating in a 32 or 64 bit environment. Determines the size of the <code class="lang-conc">size_t</code> primitive type. In order to assist with portability between 32 and 64 bit environments the <code class="lang-conc">size_t</code> primitive type - 32 bits (4 bytes being an <code class="lang-conc">int</code>) or 64 bits (8 bytes being a long) is provided. This is the type returned from functions such as <code class="lang-conc">get_global_id</code> and <code class="lang-conc">get_local_id</code>, <code class="lang-conc">sizeof</code> when used within gpu kernels and functions, is the datatype of pointers and is the type used to address arrays. This permits one to write code that is agnostic of whether one is operating in a 32 or 64 bit environment (though practically one can assume a 64 bit environment in most cases).</p>
</li>
  <li><p><b>Local Memory Size</b> - This is the amount of memory per compute unit which is accessible to a work group (collection of parallel work items) running on it, this memory can be used as a limited shared cache between the work items in a work group.</p>
</li>
  <li><p><b>Global Memory Size</b> - Note that GPUs usually have far less available to them on an individual basis than our host.</p>
</li>
  <li><p><b>Max Memory Allocation Size</b> - Not only are we limited to the Global Memory Size, but chunks of memory may be no larger than this value. This is often a value one must consider when designing an algorithm for portability, and different GPUS often have different max allocation sizes.</p>
</li>
  <li><p><b>Max Work Item Sizes</b> - When designing an algorithm which uses local memory, one must be cognizant of these values.</p>
</li>
</ul class="simple">
</div>
</div>
<h2 id="events">Events<a class="headerlink" href="#events" title="Permalink to this headline" id="events">?</a></h2>
<p>It is advantageous for computation on the GPU to take place on an concurrent basis, both for performance reasons - in allowing the GPU to reorganise execution, and for practical reasons - as we can move data to, from and between GPUs (a relatively slow operation) at the same time as they are performing execution, thus enabling us to create mini pipelines of work for our GPU, always keeping it busy, maximising throughput.</p>
<p>In Concurnas we make use of OpenCL events to support control of asynchronous computing on the GPU. These are wrapped within refs and exposed as GPURef's. Many of the core GPU operations operate on an asynchronous, non blocking basis: reading, writing and copying data between the GPU and program execution.</p>
<p>GPURefs have their status set upon completion of the task which created them.</p>
<p>The typical workflow of execution involves first copying data to the gpu, performing execution, and then copying back the result to heap managed memory. We can wait, via the /await/ keyword, on the GPURefs returned from the data writing call to ensure that execution only begins after data copying is complete, similarly, we can wait on the GPURef returned from execution before continuing to copy the result to main memory. Following on from our previous example:</p>
<pre><code class="lang-conc">inoutGPU = device.makeOffHeapArrayMixed&lt;int[]&gt;(int[].class, 10)
copyComplete boolean:gpus.GPURef = inoutGPU.writeToBuffer([1 2 3 4 5 6 7 8 9 10])
await(copyComplete)
//rest of program will execute only when copyComplete has been set..
got = inoutGPU.readFromBuffer()
</code></pre>
<p>In the above example, had we not waited for the <code class="lang-conc">copyComplete</code> ref to complete, the <code class="lang-conc">readFromBuffer</code> call may have returned an unexpected result. So by explicitly waiting we are ensuring consistency.</p>
<p>There is a way to perform the above synchronization which is generally preferred from a performance and elegance of code perspective. All the non blocking GPU operations take a vararg of GPURef's to wait for before continuing execution:</p>
<pre><code class="lang-conc">inoutGPU = device.makeOffHeapArrayMixed&lt;int[]&gt;(int[].class, 10)
copyComplete boolean:gpus.GPURef = inoutGPU.writeToBuffer([1 2 3 4 5 6 7 8 9 10])
got = inoutGPU.readFromBuffer(copyComplete)//readFromBuffer will execute when coyComplete has been set
</code></pre>
<p>In the above example the blocking of the readFromBuffer call will take place within the OpenCL framework. Generally speaking this is the preferred method of synchronization as the control takes place closer to the hardware and the code is easier to write (at least in omitting the explicit call to await).</p>
<p>If one forgets to capture the resultant GPURef from an asynchronous, non blocking GPU operation, then the operation will turn into a blocking one as the calling code will wait for the resulting GPURef to be set. This is because all the asynchronous non blocking GPU operations are marked with the <code class="lang-conc">@com.concurnas.lang.DeleteOnUnusedReturn</code> annotation.</p>
</div>
<div class="section">
<h2 id="data-transference">Data transference<a class="headerlink" href="#data-transference" title="Permalink to this headline" id="data-transference">?</a></h2>
<div class="section">
<div class="section">
<h3 id="buffers">Buffers<a class="headerlink" href="#buffers" title="Permalink to this headline" id="buffers">?</a></h3>
<p>Before we can perform calculations on the GPU we must first copy data to our GPU device. We can work with both scalar values (single values) and n dimensional arrays. Data must be of type either: primitive or boxed primitive (Integer, Character etc). We must first create an appropriate buffer to work with the data on the GPU, this is a blocking operation:</p>
<pre><code class="lang-conc">singl gpus.GPUBufferInputSingle = device.makeOffHeapArrayInSingle&lt;int&gt;(int.class)
</code></pre>
<p>Here, single has been created as an In buffer, allowing us to copy data into it only. Single and n dimensional array buffers can be created as either:</p>
<ul class="simple">
  <li><p><b>In</b>: permitting only writing of data to them, most often used for arguments to kernels (makeOffHeapArrayInSingle/makeOffHeapArrayIn)</p>
</li>
  <li><p><b>Out</b>: Permitting only reading of data from them, useful for results of kernels (makeOffHeapArrayOutSingle/makeOffHeapArrayOut)</p>
</li>
  <li><p><b>Mixed</b>: Permitting both reading and writing of data. (makeOffHeapArrayMixedSingle/makeOffHeapArrayMixed)</p>
</li>
</ul class="simple">
<p>It is advised that one chooses from the above types of buffer carefully, as it allows the GPU to optimize memory access.</p>
<p>The reified method local generic type argument is used in order to determine the size of the underlying data allocation space on the GPU.</p>
<p>Let's now create some n dimensional arrays on the GPU:</p>
<pre><code class="lang-conc">ar gpus.GPUBufferInput = device.makeOffHeapArrayIn&lt;int[]&gt;(int[].class, 10)
mat gpus.GPUBufferInput = device.makeOffHeapArrayIn&lt;int[2]&gt;(int[2].class, 2, 4)//2 by 4 matrix
</code></pre>
<p>Here we are creating both an array (ar) and a matrix (mat). Note how we must specify the dimensionality of the structure by populating the second argument of the method (which is a vararg) with its dimensions.</p>
<p>Normally, for n dimensional arrays, Concurnas uses Iliffe vectors (wikipedia link), but when working on the GPU, memory is organized in a serial manner. In practice this means is that n dimensional (where n&gt; 1) arrays cannot be irregular in shape if you wish to correctly work with them on the gpu.</p>
</div>
<div class="section">
<h3 id="writing,-reading-data">Writing, Reading Data<a class="headerlink" href="#writing,-reading-data" title="Permalink to this headline" id="writing,-reading-data">?</a></h3>
<p>All of the commands used to read, write and copy data to GPU buffers are non blocking and will return a GPURef for synchronization. They also take a list of GPURefs to wait for completion for as a final vararg.</p>
<p>Let us write some data to an In Buffer:</p>
<pre><code class="lang-conc">ar gpus.GPUBufferInput = device.makeOffHeapArrayMixed&lt;int[]&gt;(int[].class, 10)
copyComplete boolean:gps.GPURef = ar.writeToBuffer([1 2 3 4 5 6 7 8 9 10])
</code></pre>
<p>We can pass any n dimensional array of the buffer type to an In or Mixed buffer using the <code class="lang-conc">writeToBuffer</code> method. However, since the data is flattened for purposes of execution on the gpu, but we must ensure that the data passed is of equal size to that of the buffer otherwise a runtime exception will be thrown. For instance, it would be acceptable to for a 1 dimensional int array of 10 elements to be passed to a 2 dimensional array (matrix) buffer with 2 x 5 dimensionality as the data size is the same (<code class="lang-conc">10 ==  2 * 5 =&gt; 10</code> entries).</p>
<p>We can read from a buffer as follows:</p>
<pre><code class="lang-conc">result int[] = ar.readFromBuffer(copyComplete)
</code></pre>
<p>Note above by passing in the <code class="lang-conc">copyComplete</code> ref we are waiting for the write operation to first complete.</p>
<p>Single buffers are even easier to work with:</p>
<pre><code class="lang-conc">ar gpus.GPUBufferInput = device.makeOffHeapArrayMixed&lt;int&gt;(int.class)
copyComplete boolean:gps.GPURef = ar.writeToBuffer(99)
result int = ar.readFromBuffer(copyComplete)
//result = 99
</code></pre>
</div>
<div class="section">
<h3 id="copying-data">Copying Data<a class="headerlink" href="#copying-data" title="Permalink to this headline" id="copying-data">?</a></h3>
<p>We can copy data from an Out or Mixed buffer to a In or Mixed buffer using <code class="lang-conc">copyToBuffer</code>:</p>
<pre><code class="lang-conc">intoutGPU1 = device.makeOffHeapArrayMixed&lt;int[]&gt;(int[].class, 10)
intoutGPU2 = device.makeOffHeapArrayMixed&lt;int[]&gt;(int[].class, 10)
c1 := intoutGPU1.writeToBuffer([1 2 3 4 5 6 7 8 9 10])

g1 := intoutGPU1.copyToBuffer(intoutGPU2, c1)
//intoutGPU2 now contains a copy of the data asigned to intoutGPU1
</code></pre>
<p>The above copy mechanism can be used in order to copy non overlapping regions of data inside the same buffer.</p>
</div>
<div class="section">
<h3 id="writing,-reading-and-copying-subregions">Writing, Reading and Copying subregions<a class="headerlink" href="#writing,-reading-and-copying-subregions" title="Permalink to this headline" id="writing,-reading-and-copying-subregions">?</a></h3>
<p>We can copy subregions of data to and from as well as between buffers using the <code class="lang-conc">[a ... b]</code>, <code class="lang-conc">[a .. ]</code> and <code class="lang-conc">[ ... b]</code> syntax and via additional arguments to the <code class="lang-conc">copyToBuffer</code> method. When using the <code class="lang-conc">[ ... ]</code> syntax since we have no way of capturing the resultant GPURef of the write or read operation, the caller will block implicitly until this is returned, and will call delete on the GPURef object freeing the memory used for it on the GPU.</p>
<pre><code class="lang-conc">intoutGPU1 = device.makeOffHeapArrayMixed&lt;int[]&gt;(int[].class, 20)
//intoutGPU1 is inialized with all 0's

intoutGPU1[5 ... 8] = [66 55 44]//blocking operation
subWrite := intoutGPU1.subAssign(15, 18, [66 55 44])//non blocking operation

await(subWrite)

result1 = intoutGPU1[5 ... 8]//blocking subread
result2 := intoutGPU1.sub(15 18)//non blocking subread

await(result2)
</code></pre>
<p>The other variants of subregion are intuitive.</p>
<p>When copying between regions we can add additional arguments to the <code class="lang-conc">copyToBuffer</code> method:</p>
<pre><code class="lang-conc">intoutGPU1 = device.makeOffHeapArrayMixed&lt;int[]&gt;(int[].class, 10)
intoutGPU2 = device.makeOffHeapArrayMixed&lt;int[]&gt;(int[].class, 10)
c1 := intoutGPU1.writeToBuffer([1 2 3 4 5 6 7 8 9 10])

g1 := intoutGPU1.copyToBuffer(intoutGPU2, 2, 4, 2, c1)
</code></pre>
<p>Here, we are copying two items from <code class="lang-conc">intoutGPU1</code> into the 2nd index of <code class="lang-conc">intoutGPU2</code>.</p>
</div>
</div>
<h2 id="sizeof2">sizeof<a class="headerlink" href="#sizeof2" title="Permalink to this headline" id="sizeof2">?</a></h2>
<p>It can be useful to know how many bytes of memory a structure will take up on the GPU. To this end a qualifier can be specified to the <code class="lang-conc">sizeof</code> keyword as follows:</p>
<pre><code class="lang-conc">myArray = [0 1 2 3 4 5 6 7 8 9]
size = sizeof&lt;gpus.gpusizeof&gt; myArray 
//myArray == 40 bytes
</code></pre>
</div>
<div class="section">
<h2 id="kernels-and-functions">Kernels and functions<a class="headerlink" href="#kernels-and-functions" title="Permalink to this headline" id="kernels-and-functions">?</a></h2>
<p>Kernels are the core which drive our execution on the GPU. The compute model for kernels does have a number of differences relative to non gpu Concurnas code which one needs to be aware of in order to derive maximum value from them.</p>
<p>Here is an example kernel for simple matrix multiplication<sup><a href="#fn1">1</a></sup> with its conventional, non GPU counterpart on for comparison:</p>
<div class="container">
 <center>
  <div class="row">
   <div class="col" >
<pre><code class="lang-conc">gpukernel 2 matMult(wA int, wB int, global in matA float[2], global in matB float[2], global out result float[2]) {
  globalRow = get_global_id(0) // Row ID
  globalCol = get_global_id(1) // Col ID
	
  value = 0f;
  for (k = 0; k &lt; wA; ++k) {
    value += matA[globalCol * wA + k] * matB[k * wB + globalRow];
  }
	
  // Write element to output matrix
  result[globalCol * wA + globalRow] = value;
}
</code></pre>
   </div>
   <div class="col" >
<pre><code class="lang-conc">def matMult(M int, N int, K int, A float[2], B float[2], C float[2]) {
  for (m=0; m&lt;M; m++) {
    for (n=0; n&lt;N; n++) {
      acc = 0.f
      for (k=0; k&lt;K; k++) {
        acc += A[k][m] * B[n][k]
      }
      C[n][m] = acc
    }
  }
}
</code></pre>
   </div>
  </div>
 </center>
</div>
<p>The GPU kernel is only executable on a device as par below. Kernels cannot be executed via normal function invocation. GPUs operate on a single instruction, multiple data basis, thus unlike the non gpu code on the right, the two outermost for loops are not required as the kernel is executed in parallel on the cores of the GPU device executing it.</p>
<p>We can use something like the following in order to execute the kernel on the GPU (assuming we have already selected a device):</p>
<pre><code class="lang-conc">inGPU1 = device.makeOffHeapArrayIn(float[2].class, 2, 2)
inGPU2 = device.makeOffHeapArrayIn(float[2].class, 2, 2)
result = device.makeOffHeapArrayOut(float[2].class, 2, 2)

c1 := inGPU1.writeToBuffer([1.f 2.f ; 3.f 4.f])
c2 := inGPU2.writeToBuffer([1.f 2.f ; 3.f 4.f])

inst = matMult(2, 2, 2, inGPU1, inGPU2, result)//create a 'kernel' object from our gpukernel
compute := device.exe(inst, [2 2], c1, c2)//execute kernel on device, with 2 x 2 (4) work items, whilst waiting for c1 and c2 (operations to write to gpu) to complete
 ret = result.readFromBuffer(compute)//read result from output buffer, whilst waiting for gpu to finish execution of our kernel
</code></pre>
<p>We shall now look in detail at what is taking place above...</p>
<div class="section">
<div class="section">
<h3 id="work-items">Work Items<a class="headerlink" href="#work-items" title="Permalink to this headline" id="work-items">?</a></h3>
<p>The memory/execution model for OpenCL exposed within Concurnas is as follows:</p>
<center><img src="images/gpu-memModel.png"></center><p><p><center>Memory/execution model for OpenCL exposed within Concurnas</center></p></p>
<p>As shown in the image, each host machine may have many Compute devices, each made up of multiple compute units, themselves holding multiple processing elements.</p>
<p>When we execute Kernels, they are executed as work items in parallel across processing elements of the compute units (as work groups) on the GPU device(s) we invoke them on. Processing elements have their own private memory space where they store registers etc, and they have access to the local memory of the compute unit they are a part of as well as global memory. Access to these memory spaces for processing elements executing work items is exponentially slower the further one moves away from the processing element - hence, private memory access is the quickest, then local memory and then global memory, however, the amount of memory at each level available is inversely proportional to the locality. We have some respite when accessing repeated elements of global memory however as a portion of local memory (normally 16Kb of the 64Kb total) is dedicated to caching global and constant memory.</p>
<p>As we have already seen, interaction between host and compute device memory is governed via the use of buffers. Moving data from the host to the GPU is a relatively slow operation, in fact, generally speaking reading data from a GPU is approximately 10x slower than writing data. This must be factored into the design of most systems which take advantage of the GPU.</p>
<p>Each kernel instance is able to determine which unique work item it is by invoking the <code class="lang-conc">get_global_id</code> auto imported gpu function.The most common way to operate on data in parallel on the gpu is then to take this identifier and use it to select a section of data to operate on. For example, here is a very simple kernel adding two 1 dimensional arrays together, interacting with both global and constant memory:</p>
<pre><code class="lang-conc">gpukernel 1 addtwo(constant A int[], constant B int[], global out C int[]){
  id = get_global_id(0)
  C[id] =A[id] + B[id]
}
</code></pre>
<p>Each work item here is operating on one item of data (one from A, one from B and one written to C), but, as par our matrix multiplication example above, work items may address more than one item of data.</p>
<p>We generally do not need to concern ourselves with the way in which Kernels are mapped to compute units and then individual work items (as this is automatically handled) unless we are looking at optimizing code - which we shall examine later.</p>
</div>
<div class="section">
<h3 id="kernel-dimensions">Kernel dimensions<a class="headerlink" href="#kernel-dimensions" title="Permalink to this headline" id="kernel-dimensions">?</a></h3>
<p>Most GPU parallelism is performed on n-dimensional array data. N-dimensional arrays in Concurnas kernels are always flattened to 1 dimensional arrays. This is performed implicitly when data is copied into a GPU Buffer. Likewise, when data is copied out of a GPU buffer, it is converted back into its n dimensional form. Given this flattening of n dimensional array data, navigation around the array space is thus performed via arithmetic, with consideration for the dimensionality of the arrays.</p>
<p>In order to assist with this array navigation, kernels must specify a dimension parameter between 1 and 3 (set to <code class="lang-conc">2</code> in the above example). One can consider that for the case of a 2 dimensional kernel, the execution is performed as a matrix. Thus a call to <code class="lang-conc">get_global_id(0)</code> provide an x axis, and <code class="lang-conc">get_global_id(1)</code> will a y axis reference (note if the kernel dimensionality was set to 3 then we would be able to call, <code class="lang-conc">getGlobalId(3)</code> to return a z axis reference). If the kernel were specified as a 1 dimensional kernel then only a <code class="lang-conc">getGlobalId(0)</code> call would be valid.</p>
<div class="container">
 <center>
  <div class="row">
   <div class="col" >
<p>Conventional storage on the heap</p>
<center><img src="images/gpu-heap1.png"></center><p><code class="lang-conc">arr[n][m]</code></p>
   </div>
   <div class="col" >
<p>Storage on the GPU</p>
<center><img src="images/gpu-heap2.png"></center><p><code class="lang-conc">arr[n*4 + m]</code></p>
   </div>
  </div>
 </center>
</div>
<div class="container">
 <center>
  <div class="row">
<p><p><center>A 4 x 4 matrix allocated on the heap and the same matrix on the GPU.</center></p></p>
  </div>
 </center>
</div>
<p>In the image we see an example of this effect on a 4x4 matrix and how on these are addressable conventionally and on the GPU.</p>
</div>
<div class="section">
<h3 id="kernel-arguments">Kernel arguments<a class="headerlink" href="#kernel-arguments" title="Permalink to this headline" id="kernel-arguments">?</a></h3>
<p>The syntax for kernel arguments is slightly more involved than for normal function definitions in Concurnas:</p>
<center>
<p><code class="lang-conc">annotations? (global|local|constant)? (in|out)? (val|var)? name primitive_type</code></p>
</center>
<p>All kernel parameters must be of primitive time or boxed equivalent (Integer, Float etc). Any kernel parameters may be marked as either: global, local or constant or without a qualifier - those that are are treated as pointers (See Pointers ) within the body of the function:</p>
<ul class="simple">
  <li><p><code class="lang-conc">global</code> - The parameter value is held within the global memory space, it must be a pointer type (function parameters are automatically converted to this form)</p>
</li>
  <li><p><code class="lang-conc">local</code> - The parameter value is held within the local memory space specific to the compute unit the processing element operating the work item instance of the current kernel resides within. Note that it is limited in size to generally no larger than 64 Kb.</p>
</li>
  <li><p><code class="lang-conc">constant</code> - The parameter is a constant. Changes cannot be made to the value. The amount of constant space on a gpu is limited and implementation specific (see the device parameter: Constant Args and Constant Buffer Size above), though usually it's 64Kb.</p>
</li>
  <li><p>BLANK - Indicates that the parameter is private to the kernel instance and may not be shared amongst work group items. Changes can be made but they are not visible outside of the work item.</p>
</li>
</ul class="simple">
<p>Kernel parameters marked as global may optionally be marked with the type of buffer capability they expect. This further enhances the type safety provided by Concurnas when working with GPUs:</p>
<ul class="simple">
  <li><p><code class="lang-conc">in</code> - indicating that only an in or mixed buffer may be used as an invocation argument.</p>
</li>
  <li><p><code class="lang-conc">out</code> - indicating that only an out or mixed buffer may be used as an invocation argument.</p>
</li>
  <li><p>BLANK - indicating that any type of buffer (in, out or mixed) is permissible as an invocation argument.</p>
</li>
</ul class="simple">
<p>Kernel arguments may be specified as arrays of dimensionality greater than 1, however, within the kernel itself these are flattened to one dimension, as discussed in the Kernel dimensions section above, are only addressable as such. Additionally they are useable only as pointers.</p>
<p><b>Type erasure</b>. Due to the fact that generic types are erased at runtime it is not possible for Concurnas to differentiate between gpukernel function signatures differing by global/local/constant parameter qualification or in kernel dimensions. Hence the following three definitions are ambiguous:</p>
<pre><code class="lang-conc">gpukernel 1 kfunc(global A int[]){... }
gpukernel 1 kfunc(local A int[]){... }//ambiguous
gpukernel 2 kfunc(global A int[]){... }//ambiguous
</code></pre>
<p>But Concurnas is able to differentiate between signatures differing in terms of in/out parameter qualification. Hence the following is not ambiguous:</p>
<pre><code class="lang-conc">gpukernel 1 kfunc(global in A int[]){... }
gpukernel 1 kfunc(global out A int[]){... }//NOT ambiguous!
</code></pre>
</div>
<div class="section">
<h3 id="calling-functions-from-kernels">Calling functions from kernels<a class="headerlink" href="#calling-functions-from-kernels" title="Permalink to this headline" id="calling-functions-from-kernels">?</a></h3>
<p>One is not limited just to executing code within kernels. Kernels may invoke other kernels (with matching kernel dimensions), built in utility functions as well as defined callable functions declared as gpudef:</p>
<pre><code class="lang-conc">gpudef powerPlus(x int, raiseto int, plus int) =&gt; (x ** raiseto ) + plus 
</code></pre>
<p>As with kernels, input parameters may be marked as either global, local or constant or without a qualifier, they cannot be invoked like normal Concurnas functions, are type erased and they are subject to the same restrictions in their definition, as described below. <code class="lang-conc">gpudef</code> functions cannot be called by ordinary Concurnas code.</p>
<p>There are a number of essential and useful built in utility functions which are auto included for all gpu kernels and gpu functions. Two common and essential ones are:</p>
<ul class="simple">
  <li><p><code class="lang-conc">get_global_id(dim int) int</code> - returns the global id for the dimension specified</p>
</li>
  <li><p><code class="lang-conc">get_local_id(dim int) int</code> - returns the local id for the dimension specified.</p>
</li>
</ul class="simple">
<p>The full list of utility functions can be found at: <code class="lang-conc">com.concurnas.lang.gpubuiltin.conc</code></p>
</div>
<div class="section">
<h3 id="stub-functions">Stub functions<a class="headerlink" href="#stub-functions" title="Permalink to this headline" id="stub-functions">?</a></h3>
<p>Sometimes, particularly in the cases where one has existing opencl C99 compliant OpenCL C code to use in Concurnas, it can be useful to define functions as stubs and reference in this code. We can achieving this in two ways via the use of the <code class="lang-conc">com.concurnas.lang.GPUStubFunction</code> annotation as follows:</p>
<p><b>Explicit source</b>. The source parameter of the annotation consumes a String holding the C99 compliant OpenCL c code to load in place of the function definition when called by the GPU. This is a useful technique for expressing smallish blocks of code:</p>
<pre><code class="lang-conc">@GPUStubFunction(source="float plus(float a, float b){ return a + b;}")
gpudef plus(a float, b float) float
</code></pre>
<p><b>Source files</b>. The sourcefiles parameter of the annotation consumes an array of Strings referencing the files holding the C99 compliant OpenCL c code to use in place of the function definition when called by the GPU. The file referenced may be absolute or relative to the working directory:</p>
<pre><code class="lang-conc">@GPUStubFunction(sources=['./cpucode.cl', './gpuutilCode.cl'])
gpudef plus(a float, b float) float
</code></pre>
<p>The two methods above can be used together, i.e. it's possible to define both explicit source code and files to import. The Annotation may be attached to regular functions (which may optionally be declared abstract), gpudef functions and gpukernels (which must be declared abstract).</p>
<p>It's also possible to use neither explicitly defined source code or reference any source files. In this case Concurnas will assume that the dependant code has been included in a explicit source or source file definition specified in a different GPUStubFunction reference.</p>
<p>Note that Concurnas assumes that the source code provided is correct, i.e. the enhanced type safety which Concurnas provides at compile time for GPUs cannot be provided with stub functions. However, runtime safety is still provided, and invalid code within a kernel chain will be flagged in a GPUException upon invocation.</p>
</div>
<div class="section">
<h3 id="recursion1">Recursion<a class="headerlink" href="#recursion1" title="Permalink to this headline" id="recursion1">?</a></h3>
<p>We are not permitted to make use of recursion when defining gpu kernels and functions, either directly or indirectly. However, it's often rare that an algorithm genuinely requires recursion, beyond satisfying code aesthetics, in order to solve a problem. Often recursive solutions to problems can be rewritten in a non recursive, though less graceful, matter. Let us look at a classic recursive solution to the fibonacci series generation problem, in classical Concurnas on the left and in Parallel gpu Concurnas code on the right:</p>
<div class="container">
 <center>
  <div class="row">
   <div class="col" >
<pre><code class="lang-conc">def fib(n int) long{
  if(n == 0){
    return 0
  }elif(n &lt;== 2){
    return 1
  }else{
    return fib(n-2) + fib(n-1)
  }
}





.
</code></pre>
   </div>
   <div class="col" >
<pre><code class="lang-conc">gpudef fib(n int) long {
  if(n == 0){
    return 0
  }elif( n &lt;== 2){
    return 1
  }else{
    r = 0L; n1 = 1L; n2 = 1L       
    for(i = 2; i &lt; n; i++){
      r = n1 + n2
      n1 = n2
      n2 = r
    }
    return r
  }	
}
</code></pre>
   </div>
  </div>
 </center>
</div>
</div>
<div class="section">
<h3 id="function-overloading">Function overloading<a class="headerlink" href="#function-overloading" title="Permalink to this headline" id="function-overloading">?</a></h3>
<p>Although Concurnas will permit us to define more than one version of a function in an overload manner (same name, different signature), only one version of that overloaded function may be referenced in an execution of a kernel. This restriction is checked for via a compile time and runtime check.</p>
</div>
<div class="section">
<h3 id="kernel-and-function-variables">Kernel and function variables<a class="headerlink" href="#kernel-and-function-variables" title="Permalink to this headline" id="kernel-and-function-variables">?</a></h3>
<p>Similar to kernel and function parameters, variables may be qualified with <code class="lang-conc">local</code> or <code class="lang-conc">global</code>. E.g.:</p>
<pre><code class="lang-conc">local xyz3 int[] = [1 2 3] //an array defined in local memory
</code></pre>
<p>Variables qualified as <code class="lang-conc">constant</code> may only be created at the top level of the code outside of any gpu kernels or functions. They may be referenced across multiple gpu kernels or functions. For example:</p>
<pre><code class="lang-conc">constant fixedInc = 100

gpukernel 1 incrementor(global in A int[], global out B int[]) {
ida = get_global_id(0) 
B[ida] = A[ida] + fixedInc
}
</code></pre>
<p>There are a some additional caveats concerning qualified variables one must note:</p>
<ul class="simple">
  <li><p>Constant variables cannot be re-assigned to once they have been declared.</p>
</li>
  <li><p>Global variables may be of pointer type only.</p>
</li>
  <li><p>By qualifying variables in the above manner, we are indicating to our GPU that we wish the value of the variable to persist within the const, local or global memory space of the GPU. As such, once a variable has been qualified as say, global, it cannot be reassigned to a non global (practically just local) variable, as they are different physical memory spaces on the GPU.</p>
</li>
</ul class="simple">
</div>
<div class="section">
<h3 id="kernel/function-restrictions">Kernel/function restrictions<a class="headerlink" href="#kernel/function-restrictions" title="Permalink to this headline" id="kernel/function-restrictions">?</a></h3>
<p>When it comes to writing code to run on the gpu, whether it be kernels or functions, there are a number of restrictions in terms of what components of Concurnas can be used both when defining and using them. Some of these have already been elaborated. The full list is as follows:</p>
<ul class="simple">
  <li><p>GPU Kernels and functions must be declared as functions. They may not be class methods or be nested in any way.</p>
</li>
  <li><p>Only GPU Kernels and functions may only invoke one another, they cannot invoke normal Concurnas functions. Non GPU Concurnas code may not directly invoke GPU Kernels or functions. (invocation from non GPU Concurnas code is described in the section below Invoking Kernels).</p>
</li>
  <li><p>The parameters of GPU Kernels and GPU functions must all:</p>
<ul class="simple">
  <li><p>Be of primitive type (or boxed equivalent) and be of any array dimensionality (though inside the kernel this is flattened to one level if marked global, local or const)</p>
</li>
  <li><p>May not be varargs</p>
</li>
  <li><p>May not have default values</p>
</li>
  <li><p>May be marked as either global, local or constant or without a qualifier. If the parameter is qualified then it will be converted to a pointer within the body of the function. On invocation, will require a corresponding buffer object to be passed to it.</p>
</li>
  <li><p>May be marked with in or out to denote readwriteability.</p>
</li>
</ul class="simple">
</li>
  <li><p>GPU Kernels must return void.</p>
</li>
  <li><p>Variables declared outside the scope of a gpu kernel or gpu function must be declared val for them to be used inside.</p>
</li>
  <li><p>Recursion of GPU Kernels or GPU functions, either directly or indirectly is not supported.</p>
</li>
  <li><p>Overloading of GPU Kernels or GPU functions is not supported.</p>
</li>
  <li><p>The new operator may not be used - except for creating arrays.</p>
</li>
  <li><p>Arrays can only be created with dimensions expressed as constants<sup><a href="#fn2">2</a></sup>.</p>
</li>
  <li><p>Kernels must have a dimensionality expressed of between 1 and 3.</p>
<p>Note that Kernel parameters themselves may be natively of more than one dimension, when passed to a GPU buffer they will be implicitly flattened to one dimension as previously described.</p>
</li>
</ul class="simple">
<p>Additionally, the following elements of Concurnas syntax/functionality may not be utilized:</p>
<ul class="simple">
  <li><p>Objects:</p>
<ul class="simple">
  <li><p>Enumerations</p>
</li>
  <li><p>Direct or self returning dot operator</p>
</li>
  <li><p>Lambdas</p>
</li>
  <li><p>init blocks</p>
</li>
  <li><p>Method references</p>
</li>
  <li><p><code class="lang-conc">&amp;==</code> and <code class="lang-conc">&amp;&lt;&gt;</code></p>
</li>
  <li><p>Annotations</p>
</li>
  <li><p>The super keyword</p>
</li>
  <li><p>Local classes</p>
</li>
  <li><p>Nested functions</p>
</li>
  <li><p>Tuples</p>
</li>
</ul class="simple">
</li>
  <li><p>Object Providers</p>
</li>
  <li><p>Generics</p>
</li>
  <li><p>Pattern matching</p>
</li>
  <li><p>Exceptions:</p>
<ul class="simple">
  <li><p>Try catch</p>
</li>
  <li><p>Throwing of exceptions</p>
</li>
</ul class="simple">
</li>
  <li><p>Compound statements:</p>
<ul class="simple">
  <li><p>Break and continue returning a value</p>
</li>
  <li><p>Only <code class="lang-conc">for( ; ; ) </code> is valid, other variants of <code class="lang-conc">for</code> are not</p>
</li>
  <li><p>For and while blocks with else</p>
</li>
  <li><p>For and while blocks an index</p>
</li>
  <li><p>Compound statements which return a value (<code class="lang-conc">if</code>, <code class="lang-conc">while</code>, <code class="lang-conc">for</code> etc)</p>
</li>
</ul class="simple">
</li>
  <li><p>Concurrency/reactive computing:</p>
<ul class="simple">
  <li><p>The await keyword (if you want to use this then barriers are probably the solution)</p>
</li>
  <li><p>Refs</p>
</li>
  <li><p>The changed keyword</p>
</li>
  <li><p><code class="lang-conc">onchange, every, async</code></p>
</li>
  <li><p><code class="lang-conc">async, sync</code></p>
</li>
  <li><p>Actors</p>
</li>
  <li><p><code class="lang-conc">parfor, parforsync</code></p>
</li>
  <li><p>Transactions</p>
</li>
</ul class="simple">
</li>
  <li><p>Arrays/Maps:</p>
<ul class="simple">
  <li><p>Arrays of non constant size (e.g. <code class="lang-conc">new int[n]</code> where <code class="lang-conc">n</code> is a variable)</p>
</li>
  <li><p>Array default values</p>
</li>
  <li><p>The length parameter of an array is not exposed.</p>
</li>
  <li><p>List instantiation</p>
</li>
  <li><p>Empty arrays</p>
</li>
  <li><p>Maps</p>
</li>
  <li><p>Array pre/post and sublist range references</p>
</li>
  <li><p>Instantiated arrays outside of assignment statements</p>
</li>
</ul class="simple">
</li>
  <li><p>References to non constant module level, global, variables</p>
</li>
  <li><p>Other:</p>
<ul class="simple">
  <li><p>The assert keyword</p>
</li>
  <li><p>Named parameters on function invocations</p>
</li>
  <li><p><code class="lang-conc">@</code> copy</p>
</li>
  <li><p>The <code class="lang-conc">del</code> keyword</p>
</li>
  <li><p>offHeap data</p>
</li>
  <li><p>The <code class="lang-conc">in</code> or <code class="lang-conc">is</code> keywords</p>
</li>
  <li><p>the <code class="lang-conc">&gt;&gt;&gt;</code> operator (<code class="lang-conc">&lt;&lt;</code> and <code class="lang-conc">&gt;&gt;</code> are ok)</p>
</li>
  <li><p><code class="lang-conc">sizeof</code> with qualification. Normal <code class="lang-conc">sizeof</code> is ok</p>
</li>
  <li><p>string and regex declarations</p>
</li>
  <li><p>The <code class="lang-conc">with</code> keyword</p>
</li>
  <li><p>Language extensions</p>
</li>
  <li><p>Multi assign using an assignor other than <code class="lang-conc">=</code> or with <code class="lang-conc">global</code>/<code class="lang-conc">local</code>/<code class="lang-conc">constant</code> variables.</p>
</li>
  <li><p>The null safety related operators: safe call: <code class="lang-conc">?.</code>, elvis: <code class="lang-conc">?:</code> or not-null assertion: <code class="lang-conc">??</code></p>
</li>
</ul class="simple">
</li>
</ul class="simple">
<p>Concurnas handles the above restrictions at multiple levels in the programming and execution process by checking for them as part of the type system, during compilation time and finally at runtime. Unfortunately, for the C99 compatible C code expressed via stub functions, the restriction checking and error reporting, although comprehensive, is performed at runtime - which is of course not as convenient or safe as if it were to be performed exclusively at compile time.</p>
</div>
<div class="section">
<h3 id="invoking-kernels">Invoking kernels<a class="headerlink" href="#invoking-kernels" title="Permalink to this headline" id="invoking-kernels">?</a></h3>
<p>Returning to our complete matrix multiplication example above:</p>
<pre><code class="lang-conc">inGPU1 = device.makeOffHeapArrayIn(float[2].class, 2, 2)
inGPU2 = device.makeOffHeapArrayIn(float[2].class, 2, 2)
result = device.makeOffHeapArrayOut(float[2].class, 2, 2)

c1 := inGPU1.writeToBuffer([1.f 2.f ; 3.f 4.f])
c2 := inGPU2.writeToBuffer([1.f 2.f ; 3.f 4.f])

inst = matMult(2, 2, 2, inGPU1, inGPU2, result)//create a 'kernel' object from our gpukernel
compute := device.exe(inst, [2 2], c1, c2)//execute kernel on device, with 2 x 2 (4) work items, whilst waiting for c1 and c2 (operations to write to gpu) to complete
ret = result.readFromBuffer(compute)//read result from output buffer, whilst waiting for gpu to finish execution of our kernel
</code></pre>
<p>We first obtain the kernel by calling the gpukernel function like a normal function invocation, this will return a GPUKernel object which can be executed by a device. Global, local and constant kernel parameters must be satisfied with arguments that are buffers (inGPU1 and inGPU2 above). Unqualified arguments may point to non buffered variables, but note however that these are copied at invocation time to the GPU so it is advisable to keep these data structures small since as previously discussed, data transference is often the bottleneck in GPU work</p>
<p>Next we call exe on our chosen device. If this is the first time we have called the kernel on the device's parent device group, then the kernel will be compiled which takes a non epsilon amount of time. We must pass in an array, the product of which corresponds to the number of work items we wish to spawn in the number of dimensions (between 1 and 3 inclusive), corresponding to those required of our kernel. We may also optionally specify any local dimensions in the same way. Finally we can optionally pass in references to events to wait for completion of, in the above example we wait for the buffer write operations in events c1 and c2 to complete.</p>
<p>An GPURef object holding the execution completion event is returned from the exe method. Execution of a kernel is said to have completed one all of the work items have completed execution. As with buffer operations, kernel execution is a non blocking asynchronous operation, so we must wait on this object is appropriate.</p>
</div>
</div>
<h2 id="profiling">Profiling<a class="headerlink" href="#profiling" title="Permalink to this headline" id="profiling">?</a></h2>
<p>The Concurnas GPU implementation provides detailed profiling options. This is useful for both development, in enabling one to debug one's GPU code so as to determine how much time is spent performing certain operations, and for monitoring purposes. GPURef objects created from asynchronous non blocking gpu operations have a getProfilingInfo method, which returns an object of type GPUProfilingInfo class holding all the profiling information available:</p>
<pre><code class="lang-conc">c1 := inGPU1.writeToBuffer([ 1 2 3 4 ])
copyduration = c1.getProfilingInfo().getWorkDuration()
summary = c1.getProfilingInfo().toString()
</code></pre>
<p>In the above example, the value returned from the getWorkDuration method call is a delta in nanoseconds (divide this by 1e6 to obtain a millisecond value).</p>
</div>
<div class="section">
<h2 id="pointers">Pointers<a class="headerlink" href="#pointers" title="Permalink to this headline" id="pointers">?</a></h2>
<p>Since when working with GPUs we are working closer to the metal than ordinary Concurnas code, when writing gpu kernels and gpu functions we are afforded access to the use of pointers. These operate the same as pointers in low level languages such as C and C++ and can be very useful for defining programs to operates on the GPU or porting over existing code to operate within the Concurnas framework and not having to rely on GPUFunctionStubs (and the reduced amount of type safety stubs expose). Pointers are especially useful when working with subregions of global memory variables.</p>
<p>A pointer is a variable which contains the memory address of a different variable. We can have a pointer to any variable type. Pointer types are defined as follows:</p>
<center>
<p><code class="lang-conc"> (*+)ordinaryType </code></p>
</center>
<p>Example pointer types:</p>
<pre><code class="lang-conc">*int //a pointer to an integer
*float//a pointer to a float
**int//a pointer to a pointer to an integer
***int//a pointer to a pointer to a pointer to an integer
</code></pre>
<p>The <code class="lang-conc">~</code> operator provides the address of a variable:</p>
<center>
<p><code class="lang-conc"> ~variable </code></p>
</center>
<p>We can use the <code class="lang-conc">~</code> operator in order to create pointers like this:</p>
<pre><code class="lang-conc">normalVar int = 12
pnt *int  = ~normalVar //pointer holding the address of normalVar 
pnt2pnt **int = ~pnt //pointer holding the address of the pointer to normalVar 
</code></pre>
<p>The type upon which we apply the address <code class="lang-conc">&amp;</code> operator must match that to which the pointer type refers. For example:</p>
<pre><code class="lang-conc">aint = 12
afloat = 12.f

pnt1 *int = ~aint //ok int == int
pnt2 *int = ~afloat //error, type mismatch, int &lt;&gt; float
</code></pre>
<p>If we want to obtain the value pointed to by a pointer, we prefix the variable with the dereference operator /*/. We can prefix it for as many times as we require dereferencing:</p>
<pre><code class="lang-conc">normalVar = 12
pnt  = ~normalVar 
pnt2pnt  = ~pnt 

//now for the dereferencing:

normalVarAgain int = *pnt 
aPointer *int = *pnt2pnt
normalVarOnceMore int = **pnt2pnt
</code></pre>
<p>In order to disambiguate pointer dereferencing from the multiplication operator, parentheses must be used:</p>
<pre><code class="lang-conc">normalVar = 12
pnt  = ~normalVar 
pow2 = (*pnt) * (*pnt)
</code></pre>
<p>The dereferencing operation is similar for when we want to set the value pointed to by a of a pointer:</p>
<pre><code class="lang-conc">normalVar = 12
pnt *int = ~normalVar 
*pnt = 99 //the variable normalVar now equals 99
</code></pre>
<div class="section">
<div class="section">
<h3 id="pointers-and-arrays">Pointers and Arrays<a class="headerlink" href="#pointers-and-arrays" title="Permalink to this headline" id="pointers-and-arrays">?</a></h3>
<p>Pointers and arrays in gpu kernels and gpu functions are two closely related concepts. We can set the value of a pointer to be the address of an array with element offset (remember, we count from 0) as follows:</p>
<pre><code class="lang-conc">normalArray = [1 2 3 4 5]
pnt1  *int = ~normalArray[0] //pnt1 is set to the address of the first (start) element of the array
pnt2  *int = ~normalArray[1] //pnt1 is set to the address of the second element of the array
</code></pre>
<p>An especially useful feature of pointers to arrays (and strictly speaking, pointers in general) is the ability to perform pointer arithmetic and use them, syntactically, as one dimensional arrays:</p>
<pre><code class="lang-conc">normalArray = [1 2 3 4 5]
pnt1  *int = ~normalArray[0]
pnt2  *int = ~normalArray[1]

//arithmetic...

pnt1++//pnt1 now points to the second entry in normalArray
avalue = *(pnt1+1)//avalue now equals the 3rd value from normalArray, 3
*(pnt2+1) = 100//set the 3rd value of normalArray to 100
extracted = pnt2[3]//obtain the 3rd value of normalArray, which is now 100
</code></pre>
<p>Multi dimensional arrays can be operated on in a similar way:</p>
<pre><code class="lang-conc">myAr = new int[3, 4] //a matrix
myAr[0, 0] = 12
myAr[0, 1] = 14

pnt *int = ~myAr[0][0]//set pnt to point to the first value of the matrix
pnt += 1//pnt now points to the second value of the first row of the matrix

value = *pnt//value is now 14
</code></pre>
</div>
<div class="section">
<h3 id="array-of-pointers">Array of pointers<a class="headerlink" href="#array-of-pointers" title="Permalink to this headline" id="array-of-pointers">?</a></h3>
<p>Sometimes it can be useful to make use of an array of pointers. This can be defined as follow:</p>
<pre><code class="lang-conc">a = 44
b = 88
c = 23

pntc = ~c
arOfPnt *int[] = [~a ~b pntc]//array of pointers
*arOfPnt[1] = 99	//b now equals 88
</code></pre>
</div>
</div>
<h2 id="local-memory">Local memory<a class="headerlink" href="#local-memory" title="Permalink to this headline" id="local-memory">?</a></h2>
<p>It is expensive to read and write to global memory, thus for algorithms involving a  high degree of spatial locality, using local memory can greatly improve processing time performance. This is achieved by essentially manually caching the data of interest to a work group (composed of a number of work items operating on processing units) within the local memory region specific to the compute unit work group. Here we will examine how this can be achieved.</p>
<p>Data that is in local memory is shared between all work items in a work group. Recall that compute units on a gpu execute the work items as part of a work groups on their processing elements. This interaction usually needs to be synchronised by using barriers. Work items in different work groups (resident therefore on different compute units) cannot share information via local memory - this has to be achieved via using global memory.</p>
<div class="section">
<div class="section">
<h3 id="local-buffers">Local Buffers<a class="headerlink" href="#local-buffers" title="Permalink to this headline" id="local-buffers">?</a></h3>
<p>We can specify local kernel parameters by qualifying them with <code class="lang-conc">local</code>. When doing so they must be satisfied with <code class="lang-conc">gpus.Local</code> buffers on kernel invocation as follows:</p>
<center>
<p><code class="lang-conc">localBuffer = gpus.Local(long[].class, localItemSize)</code></p>
</center>
<p>Essentially we are just providing the type and buffer size information. Local buffers provide no means to write into/out of them outside the GPU. This of course means that local kernel parameters may not be further qualified as being <code class="lang-conc">in out</code> parameters.</p>
<p>The amount of local memory available to a work group executing on a compute unit is limited - generally it is no more than 64Kb, to be shared across multiple buffers. To see how much local memory is available the <code class="lang-conc">device.getLocalMemSize()</code> function can be called.</p>
</div>
<div class="section">
<h3 id="barriers">Barriers<a class="headerlink" href="#barriers" title="Permalink to this headline" id="barriers">?</a></h3>
<p>Since we are working very close to the metal with GPUs, synchronization of parallel work items running on a GPU compute unit is slightly more involved than what we are used to elsewhere in Concurnas. We use barriers in order to ensure that all work items executing on a processing element have finished writing to local (or global) memory. This is essential for algorithms which use local memory on a reductive/iterative basis.</p>
<p>We achieve this by using the barrier gpudef function:</p>
<ul class="simple">
  <li><p><b>barrier(true)</b> - The barrier function will either flush any variables stored in local memory or queue a memory fence to ensure correct ordering of memory operations to local memory.</p>
</li>
  <li><p><b>barrier(false)</b> - The barrier function will queue a memory fence to ensure correct ordering of memory operations to global memory. This can be useful when work-items, for example, write to buffer and then want to read the updated data.</p>
</li>
</ul class="simple">
</div>
<div class="section">
<h3 id="local-work-size">Local Work Size<a class="headerlink" href="#local-work-size" title="Permalink to this headline" id="local-work-size">?</a></h3>
<p>When invoking gpukernels using local memory we must specify the dimensions used for our local identifiers accessible by the work items running as part of a work group. We do this in much the same way as we defined global dimensions on invocation; we pass in an array of dimensions. There are however some caveats we must be aware of. The specified local dimensions must:</p>
<ul class="simple">
  <li><p>Evenly divide into the global dimensions specified.</p>
</li>
  <li><p>Be no larger than the device specific Max Work Item Sizes per dimension (up to 3). This can be determined at runtime by calling <code class="lang-conc">device.getMaxWorkItemSizes()</code>.</p>
</li>
</ul class="simple">
<p>Due to the requirement that local work size dimensions evenly divide into global dimensions sometimes it means that data will have to be padded or the algorithm used otherwise adjusted.</p>
<p>A work item can determine its local id by using the <code class="lang-conc">get_local_id</code> gpudef function. It is also often useful to know the size of the local work size, the <code class="lang-conc">get_local_size</code> gpudef function provides this.</p>
<p>Concurnas uses OpenCL to achieve GPU parallelism, and this has been designed to be agonistic of the number of compute units available for performing execution, nevertheless it is useful to know what this is and calling <code class="lang-conc">device.getComputeUnits()</code> will return this.</p>
</div>
<div class="section">
<h3 id="example-kernel-using-local-parameters">Example Kernel using local Parameters<a class="headerlink" href="#example-kernel-using-local-parameters" title="Permalink to this headline" id="example-kernel-using-local-parameters">?</a></h3>
<p>Reduction is an important algorithmic concept which is often encountered when building parallel algorithms for running on GPUs. Often it is used in order to derive a singular or reduced summary value from previous calculations. Here we examine a reduction algorithm with calculates the sum of long values in an array.</p>
<p>This diagram illustrates the general algorithmic approach and how local memory fits into this:</p>
<center><img src="images/gpu-reduction.png"></center><p><p><center>Reduction algorithm with multiple local memory strides</center></p></p>
<p>In the image we see a visual representation of the reduction algorithm. First we copy our data into global memory, then we pass over to the algorithm above. Once there each of the items in our work group copy the segment of data they are working on into local memory, before iteratively reducing the data in that buffer by half on each round of summation until only one value in local buffer array slot zero remains. This is then written into global memory and the final summation of these values (one for each work group) calculated on the CPU. Note that we set the local work group to size 16 in order to make it easier to show on the diagram, but in practice this value will be much larger.</p>
<p>Given that we are reducing by half on every iteration it is important for this algorithm that the number of work items in a work group be a power of two. This is of course not a requirement for all algorithms taking advantage of local memory.</p>
<p>Now let us look at the kernel, invocation and supporting function code:</p>
<pre><code class="lang-conc">gpukernel 1 reduce(global val input long[], global partialSums long[], local localSums long[]){
  local_id = get_local_id(0)
  group_size = get_local_size(0)
	
  // Copy from global memory to local memory
  localSums[local_id] = input[get_global_id(0)]
	
  // Loop for computing localSums
  stride = group_size/2
  while(stride&gt;0) {
    // Waiting for each 2x2 addition into given workgroup
    barrier(true)
    // Divide WorkGroup into 2 parts and add elements 2 by 2
    // between local_id and local_id + stride
    if (local_id &lt; stride){
      localSums[local_id] += localSums[local_id + stride]
    }
    stride/=2
  }
	
  if(local_id == 0){ // Write result into global memory
    partialSums[get_group_id(0)] = localSums[0]
  }
}

def chooseGPUDeviceGroup(gpuDev gpus.DeviceGroup[], cpuDev gpus.DeviceGroup[]) {
  if(not gpuDev){//no gpu return first cpu
    cpuDev[0]
  }else{//intel hd graphics not as good as nvidea dedicated gpu or ati so deprioritize
    for(grp in gpuDev){
      if('Intel' not in grp.platformName){
        return grp
      }
    }
    gpuDev[0]//intel device then...
  }
}

def makeData(itemcount int, range = 3){
  data = new long[itemcount]
	
  random = new java.util.Random(654L)
  for(i = 0; i &lt; itemcount; i++){
    data[i] = random.nextInt(range)//output: 0,1 or 2 with default range = 3
  }
  data
}

def sum(inputs long...){
  ret = 0
  for(i in inputs){
    ret += i
  }
  ret
}


def main(){
  gps = gpus.GPU()
  deviceGrp = chooseGPUDeviceGroup(gps.getGPUDevices(), gps.getCPUDevices())
  device = deviceGrp.devices[0]
	
  localItemSize = device.getMaxWorkItemSizes()[0] as int //algo requires local memory to be power of two size,
  //the max first dimention gpu size is a factor of two so we use this 
	
  InputDataSize = 1000*localItemSize as int
  System.\out.println("InputDataSize: {InputDataSize} elements")
	
  inputdata = device.makeOffHeapArrayIn(long[].class, InputDataSize)
  partialSums = device.makeOffHeapArrayMixed(long[].class, InputDataSize/localItemSize)
  localSums = gpus.Local(long[].class, localItemSize)
  data = makeData(InputDataSize)
  c1 := inputdata.writeToBuffer(data)//create some psudo random data
	
  inst = reduce(inputdata, partialSums, localSums)
  compute := device.exe(inst, [InputDataSize], [localItemSize], c1)//we must pass in the local item size for each work group
  ret = partialSums.readFromBuffer(compute)
	
  ctime = c1.getProfilingInfo().toString()
	
  //cleanup  
  del inputdata, partialSums
  del c1, compute
  del deviceGrp, device
  del inst
	
  restum = sum(ret)//sum of the partialSums in the  array slots to get our result
  verify = sum(data)//verify result on CPU
	
  System.\out.println('PASS! result: {restum}' if restum == verify else 'FAIL! result {restum} vs {verify}')
  ""
}

///////////
//Output:
//InputDataSize: 1024000 elements
//PASS! result: 1024399

//Assuming execution is taking place on a gpu with the first MaxWorkItemSize elemnt of 1024 .
</code></pre>
<p>The only significant change we make here in terms of execution vs a kernel with only global memory interaction is in having to provide a local work size dimension array, in the same way as we do global dimensions - <code class="lang-conc">device.exe(inst, [ItemCount], [localItemSize], c1)</code>.</p>
<p>In the final phase of the algorithm above we are obtaining the resultant value by summing the values within the array read from our GPU. It is possible to perform this final step on the GPU but for the purposes of clarity in this example this has been excluded.</p>
</div>
<div class="section">
<h3 id="example-kernel-using-local-variables">Example Kernel using local Variables<a class="headerlink" href="#example-kernel-using-local-variables" title="Permalink to this headline" id="example-kernel-using-local-variables">?</a></h3>
<p>Let us now revisit our matrix multiplication example examined previously and see if we can improve performance by using local memory, specifically local variables...</p>
<center><img src="images/gpu-matmult1.png"></center><p><p><center>A naive algorithm for matrix multiplication</center></p></p>
<p>We see in the image that each resulting cell is the product of each row of A and column of B summed together. We can iterate in a couple loop through each row and column of A and B respectfully. The code implementing this algorithm on the GPU and CPU is very similar and can be seen in functions <code class="lang-conc">matMultNaive</code> and <code class="lang-conc">matMultOnCPU</code>. It turns out that this algorithm expresses a high degree of spatial locality since a row of C is dependant on every value in B and a row of A, hence if we dedicate calculation of an element in C to a work item in our GPU we end up loading the values in A and B many multiples of times. In fact, relative to the amount of work done simply loading data (a relatively slow operation) we spend very little time doing actual computation.</p>
<p>We can improve on this algorithm by making use of a local cache as follows:</p>
<center><img src="images/gpu-matmult2.png"></center><p><p><center>An algorithm for matrix multiplication utilizing local memory cache.</center></p></p>
<p>In the image we see that we can calculate a block of values of C by caching in local memory two equal sized segments from A and B. The overall calculation performed by our work group is shown in the image:</p>
<center><img src="images/gpu-matmult3.png"></center><p><p><center>A calculation of a region of our output array C</center></p></p>
<p>We can see how an individual value of C is from the local cache in the image.</p>
<center><img src="images/gpu-matmult3.png"></center><p><p><center>Calculation of a single element of the range in C</center></p></p>
<p>Let us now look at the code which implements this, in function: <code class="lang-conc">matMultLocal</code>. Below the naive and cpu implementation of matrix multiplication are included for sake of comparison. We square a 128 by 128 float element matrix populated with pseudo random data:</p>
<pre><code class="lang-conc">def matMultOnCPU(M int, N int, K int, A float[2], B float[2], C float[2]){
  for (m=0; m&lt;M; m++) {
    for (n=0; n&lt;N; n++) {
      acc = 0.f
      for (k=0; k&lt;K; k++) {
        acc += A[k][m] * B[n][k]
      }
      C[n][m] = acc
    }
  }
}

gpukernel 2 matMultNaive(M int, N int, K int, constant A float[2], constant B float[2], global out C float[2]) {
  globalRow = get_global_id(0) 
  globalCol = get_global_id(1) 
	
  acc = 0.0f;
  for (k=0; k&lt;K; k++) {
    acc += A[k*M + globalRow] * B[globalCol*K + k]
  }
	
  C[globalCol*M + globalRow] = acc;
}

val CacheSize = 16

gpukernel 2 matMultLocal(M int, N int, K int, constant A float[2], constant B float[2], global out C float[2]) {
  row = get_local_id(0)
  col = get_local_id(1)
  globalRow = CacheSize*get_group_id(0) + row //row of C (0..M)
  globalCol = CacheSize*get_group_id(1) + col //col of C (0..N)
	
  //local memory holding cache of CacheSize*CacheSize elements from A and B
  local cacheA = float[CacheSize, CacheSize]
  local cacheb = float[CacheSize, CacheSize]
	
  acc = 0.0f
	
  //loop over all tiles
  cacheSize int = K/CacheSize
  for (t=0; t&lt;cacheSize; t++) {
    //cache a section of A and B from global memory into local memory 
    tiledRow = CacheSize*t + row
    tiledCol = CacheSize*t + col
    cacheA[col][row] = A[tiledCol*M + globalRow]
    cacheb[col][row] = B[globalCol*K + tiledRow]
		
    barrier(true)//ensure all work items finished caching
		
    for (k=0; k&lt;CacheSize; k++) {//accumulate result for matrix subsections
      acc += cacheA[k][row] * cacheb[col][k]
    }
		
    barrier(true)//ensure all work items finished before moving on to next cache section
  }
	
  C[globalCol*M + globalRow] = acc
}

def chooseGPUDeviceGroup(gpuDev gpus.DeviceGroup[], cpuDev gpus.DeviceGroup[]) {
  if(not gpuDev){//no gpu return first cpu
    cpuDev[0]
  }else{//intel hd graphics not as good as nvidea dedicated gpu or ati so deprioritize
    for(grp in gpuDev){
      if('Intel' not in grp.platformName){
        return grp
      }
    }
    gpuDev[0]//intel device then...
  }
}

def createData(xy int, range = 10){
  ret = float[xy, xy]
  random = new java.util.Random(654L)
  for(n = 0; n &lt; xy; n++){
    for(m = 0; m &lt; xy; m++){
      ret[n,m] = random.nextInt(range+1)//1. ... 10. inclusive
    }
  }
  ret
}

def main(){
  gps = gpus.GPU()
  cpus = gps.getCPUDevices()
  deviceGrp = chooseGPUDeviceGroup(gps.getGPUDevices(), cpus)
  device = deviceGrp.devices[0]
	
  xy = 128//assume square matrix with 128 * 128 elements
	
  matrixA = device.makeOffHeapArrayIn(float[2].class, xy, xy)
  resultNaive = device.makeOffHeapArrayOut(float[2].class, xy, xy)
  resultLocal = device.makeOffHeapArrayOut(float[2].class, xy, xy)
	
  data = createData(xy)//blocking operation
	
  matrixA.writeToBuffer(data)
	
  inst1 = matMultNaive(xy, xy, xy, matrixA, matrixA, resultNaive)
  inst2 = matMultLocal(xy, xy, xy, matrixA, matrixA, resultLocal)
  comp1 := device.exe(inst1, [xy xy], null)
  comp2 := device.exe(inst2, [xy xy], [CacheSize CacheSize])
	
  resNative = resultNaive.readFromBuffer()
  resLocal = resultLocal.readFromBuffer()
	
  conventional = float[xy, xy]
  tick = System.currentTimeMillis()
  matMultOnCPU(xy, xy, xy, data, data, conventional)
  toc = System.currentTimeMillis()
	
  System.\out.println('Time to compute on CPU: ' + (toc - tick) + "ms")
  System.\out.println('Naive ' + ('PASS ' if resNative == conventional else 'FAIL ') + comp1.getProfilingInfo())
  System.\out.println('Local ' + ('PASS ' if resLocal == conventional else 'FAIL ') + comp2.getProfilingInfo())
	
  //cleanup
  del matrixA, resultNaive, resultLocal
  del deviceGrp, device
  del inst1, inst2
  del comp1, comp2
}
</code></pre>
<p>Output when running on a single core of a Intel Core i7-3770K (quad core) CPU and 480 parallel work items of one NVIDIA GeForce GTX 570 (i.e. of a comparable generation):</p>
<pre><code class="lang-conc">Time to compute on CPU: 20ms
Naive PASS Work Duration: 0.293ms
Local PASS Work Duration: 0.051ms
</code></pre>
<p>Observe that we create the local cache, local variable, within the <code class="lang-conc">matMultLocal</code> kernel itself instead of passing it in.</p>
<p>What is interesting here is the effect of our caching optimization. Armed with our profiling information (see above), we can measure the impact of this. Execution on a single core CPU takes 20 milliseconds, our naive implementation on the other hand takes 0.293 of a millisecond which is 68x quicker. We achieve a further improvement of almost 6x on this when we use local memory implementation. This brings our overall speed improvement to almost 400x over a single core CPU! Certainly worth the extra work.</p>
<p>Of course, there are further optimizations we can add to further increase this, and the algorithm needs to be adjusted to deal with inputs which don't divide nicely into our cache size, but for the sake of brevity we shall leave this here.</p>
</div>
</div>
<h2 id="explicit-memory-management">Explicit memory management<a class="headerlink" href="#explicit-memory-management" title="Permalink to this headline" id="explicit-memory-management">?</a></h2>
<p>Since we are working with non heap managed memory on the GPU, and because there generally is very little of it (relative to general purpose RAM where our heap resides, and other persistent storage such as disk drives) it is impractical to offer garbage collection of it. Thus we must explicitly delete/free GPU resources either by calling the delete method on the a relevant object, or by using the del keyword. Concurnas' Garbage collection mechanism will not automatically free these resources.</p>
<p>The types of object which we must explicitly free one we have finished with them are:</p>
<ul class="simple">
  <li><p><b>Buffers</b>: <code class="lang-conc">gpus.GPUBufferManagedInput</code>, <code class="lang-conc">gpus.GPUBufferManagedOutput</code>, <code class="lang-conc">gpus.GPUBufferManagedMixedSingle</code>, <code class="lang-conc">gpus.GPUBufferManagedInSingle</code>, <code class="lang-conc">gpus.GPUBufferManagedOutSingle</code>, <code class="lang-conc">gpus.GPUBufferManagedMixedSingle</code>. Note that Local buffers do not need deletion as they are not allocated nor allocatable outside of the GPU.</p>
</li>
  <li><p><b>DeviceGroups</b>: <code class="lang-conc">gpus.DeviceGroup</code></p>
</li>
  <li><p><b>Devices</b>: <code class="lang-conc">gpus.Device</code></p>
</li>
  <li><p><b>Kernels</b>: <code class="lang-conc">gpus.Kernel</code></p>
</li>
  <li><p><b>GPURefs</b>: Returned from asynchronous non blocking calls from:</p>
<ul class="simple">
  <li><p>All Buffer memory interaction functions.</p>
</li>
  <li><p>Device kernel execution <code class="lang-conc">Device.exe(...)</code></p>
</li>
</ul class="simple">
</li>
</ul class="simple">
<p>In the case of GPURefs. If one deliberately negates or forgets to capture the resultant GPURef from an asynchronous non blocking GPU call then in addition to the calling code waiting for the call to complete (thus turning the call into a blocking call), the delete method will be called on the GPURef upon completion, thus freeing the memory allocated to manage the ref. This is a deliberate and very convenient mechanism for avoiding memory leaks.</p>
<p>This can be achieved either by explicitly calling the delete method on these objects, or by using the del keyword.</p>
<pre><code class="lang-conc">device.delete()
del deviceGroup, kernel
</code></pre>
</div>
<div class="section">
<h2 id="finishing">Finishing<a class="headerlink" href="#finishing" title="Permalink to this headline" id="finishing">?</a></h2>
<p>It can be useful to explicitly wait for a device to finish all current queued execution (buffer reading, writing, kernel execution), the finish method can be invoked for this purpose which will block until all work has been completed:</p>
<pre><code class="lang-conc">device.finish()
</code></pre>
<p>Calling del on a device will also implicitly call finish, thus ensuring that all work on the device has been completed before being freed.</p>
</div>
<div class="section">
<h2 id="double-precision">Double Precision<a class="headerlink" href="#double-precision" title="Permalink to this headline" id="double-precision">?</a></h2>
<p>Concurnas permits the use of single (float) and double (double) precision floating point types in gpu kernels and gpu functions<sup><a href="#fn3">3</a></sup>. It should be noted however that operations  performed on double precision type are considerably slower than the equivalent with float type. Thus when possible one should strive to use floating point type over double.</p>
<p>When creating arrays care should be taken to explicitly declare floating point values as float as the default instantiation type for a floating point literal values is double.</p>
</div>
<div class="section">
<h2 id="concurrent-use-of-gpus">Concurrent use of GPUs<a class="headerlink" href="#concurrent-use-of-gpus" title="Permalink to this headline" id="concurrent-use-of-gpus">?</a></h2>
<p>The GPU interaction related classes under <code class="lang-conc">com.concurnas.lang.gpus</code> (including, buffers, device groups, devices, kernels and gpurefs) are marked as transient.This is deliberate as it prevents accidental sharing of gpu related objects between differing iso's, which, if permitted would make state management challenging in most systems. It also avoids problems with attempting to persist gpu objects off heap, which doesn't make sense for gpu interaction objects.</p>
<p>In cases where multiple iso's require access to the GPUs at the same time, using an actor is advised, this permits one a fine degree of control over the GPUs available both from the perspective of pipelining of requests to them, and from the perspective of easily managing memory.</p>
</div>
<div class="section">
<h2 id="notes">Notes<a class="headerlink" href="#notes" title="Permalink to this headline" id="notes">?</a></h2>
<p>The reader who is aware of the inner workings OpenCL will no doubt find many aspects of the structure of the Concurnas implementation of GPU computing very familiar. This is deliberate. By not deviating to far away from "the way things are done" currently with the API exposed in raw OpenCL we hope to make GPU computing something which is natural to do for both OpenCL veterans, whilst simplifying the implementation details enough to make working with GPUs easy for newcomers.</p>
<p>Additionally, in the interests portability, we have omitted the tools and techniques exposed in OpenCL from version 2.0 onwards due, mainly to lack of compatibility with NVidia drivers, which for most of the older GPUs are only OpenCL 1.2 compliant.</p>
</div>
</div>
<hr></hr><h3>Footnotes</h3>
<small><p><sup><a id="fn1"></a>1</sup>Those readers familiar with the intricacies of GPU optimization will recognize this as a naive, unoptimized, implementation. This is intentional and we shall cover some details of Kernel optimization later.</p>
<p><sup><a id="fn2"></a>2</sup>These are created on the stack and not the heap, unlike conventional arrays in Concurnas</p>
<p><sup><a id="fn3"></a>3</sup>Normally, OpenCL implementations require an explicit declaration to perform operations on data of double precision type. But in Concurnas this is provided automatically, no explicit declaration is needed.</p>
</small>
                            </div>
                        </div>

                    </div>

                </div>


            <footer>
                <center>
                    <div class="copyright" role="contentinfo">
                        <small>&copy; 2018<script>new Date().getFullYear()>2018&&document.write("-"+new Date().getFullYear());</script> Concurnas Ltd. All rights reserved. <a href="../legal/tos.html">Terms of Service</a> | <a href="../legal/privacy.html">Privacy Policy</a> | <a href="../legal/cookies.html">Cookie Policy</a></small>           
                        </p>
                    </div>
                </center>
            </footer>

            </div>


        <nav data-toggle="wy-nav-shift" class="wy-nav-side">
            <div class="wy-side-scroll">
                <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">

                    <ul>
                        <li class="toctree-l1"><a class="reference internal" href="manual.html">Introduction</a></li>
                    </ul>
<p class="caption"><span class="caption-text">The Basics</span></p>
<ul>
 <li class="toctree-l1"><a class="reference internal" href="theBasics.html">Overview</a></li>
 <li class="toctree-l1"><a class="reference internal" href="variableAssignment.html">Variable Assignment</a></li>
 <li class="toctree-l1"><a class="reference internal" href="types.html">Types</a></li>
 <li class="toctree-l1"><a class="reference internal" href="nullsafe.html">Null Safety</a></li>
 <li class="toctree-l1"><a class="reference internal" href="tuples.html">Tuples</a></li>
 <li class="toctree-l1"><a class="reference internal" href="typedefs.html">Typedefs</a></li>
 <li class="toctree-l1"><a class="reference internal" href="multitypes.html">Multitypes</a></li>
 <li class="toctree-l1"><a class="reference internal" href="castingAndCheckingTypes.html">Casting and Checking Types</a></li>
 <li class="toctree-l1"><a class="reference internal" href="classKeyword.html">The class keyword</a></li>
 <li class="toctree-l1"><a class="reference internal" href="imports.html">Imports</a></li>
 <li class="toctree-l1"><a class="reference internal" href="operators.html">Operators</a></li>
 <li class="toctree-l1"><a class="reference internal" href="controlStatements.html">Control Statements</a></li>
 <li class="toctree-l1"><a class="reference internal" href="listComprehensions.html">List Comprehensions</a></li>
 <li class="toctree-l1"><a class="reference internal" href="exceptions.html">Exceptions</a></li>
 <li class="toctree-l1"><a class="reference internal" href="arraysMatricesAndLists.html">Arrays Matrices and Lists</a></li>
 <li class="toctree-l1"><a class="reference internal" href="maps.html">Maps</a></li>
 <li class="toctree-l1"><a class="reference internal" href="delete.html">Delete</a></li>
 <li class="toctree-l1"><a class="reference internal" href="functions.html">Functions</a></li>
</ul><p class="caption"><span class="caption-text">Object-oriented</span></p>
<ul>
 <li class="toctree-l1"><a class="reference internal" href="classes.html">Classes</a></li>
 <li class="toctree-l1"><a class="reference internal" href="traits.html">Traits</a></li>
 <li class="toctree-l1"><a class="reference internal" href="specialClasses.html">Special Classes</a></li>
 <li class="toctree-l1"><a class="reference internal" href="accessModifiers.html">Accessibility Modifiers</a></li>
 <li class="toctree-l1"><a class="reference internal" href="generics.html">Generics</a></li>
 <li class="toctree-l1"><a class="reference internal" href="enums.html">Enumerations</a></li>
 <li class="toctree-l1"><a class="reference internal" href="annotations.html">Annotations</a></li>
 <li class="toctree-l1"><a class="reference internal" href="copy.html">Copying Objects</a></li>
 <li class="toctree-l1"><a class="reference internal" href="objProviders.html">Object Providers</a></li>
</ul><p class="caption"><span class="caption-text">Advanced Techniques</span></p>
<ul>
 <li class="toctree-l1"><a class="reference internal" href="vectorization.html">Vectorization</a></li>
 <li class="toctree-l1"><a class="reference internal" href="ranges.html">Ranges</a></li>
 <li class="toctree-l1"><a class="reference internal" href="datautils.html">Datautils</a></li>
 <li class="toctree-l1"><a class="reference internal" href="patternMatching.html">Pattern Matching</a></li>
 <li class="toctree-l1"><a class="reference internal" href="methodReferences.html">Method References</a></li>
 <li class="toctree-l1"><a class="reference internal" href="offHeap.html">Off Heap Memory</a></li>
 <li class="toctree-l1"><a class="reference internal" href="expressionLists.html">Expression lists</a></li>
 <li class="toctree-l1"><a class="reference internal" href="extensionFunctions.html">Extension functions</a></li>
 <li class="toctree-l1"><a class="reference internal" href="langExtensions.html">Language Extensions</a></li>
</ul><p class="caption"><span class="caption-text">Concurrent, Distributed and GPU</span></p>
<ul class="current">
 <li class="toctree-l1"><a class="reference internal" href="concurrentProgramming.html">Concurrent Programming</a></li>
 <li class="toctree-l1 current"><a class="reference internal current" href="gpuParallelProgramming.html">GPU/Parallel programming</a>
   <ul>
   <li class="toctree-l2"><a class="reference internal" href="#prerequisites">Prerequisites</a>
   <ul>
   <li class="toctree-l3"><a class="reference internal" href="#code-portability">Code Portability</a>
   </li>
   </ul>
   </li>
   <li class="toctree-l2"><a class="reference internal" href="#events">Events</a>
   </li>
   <li class="toctree-l2"><a class="reference internal" href="#data-transference">Data transference</a>
   <ul>
   <li class="toctree-l3"><a class="reference internal" href="#buffers">Buffers</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#writing,-reading-data">Writing, Reading Data</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#copying-data">Copying Data</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#writing,-reading-and-copying-subregions">Writing, Reading and Copying subregions</a>
   </li>
   </ul>
   </li>
   <li class="toctree-l2"><a class="reference internal" href="#sizeof2">sizeof</a>
   </li>
   <li class="toctree-l2"><a class="reference internal" href="#kernels-and-functions">Kernels and functions</a>
   <ul>
   <li class="toctree-l3"><a class="reference internal" href="#work-items">Work Items</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#kernel-dimensions">Kernel dimensions</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#kernel-arguments">Kernel arguments</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#calling-functions-from-kernels">Calling functions from kernels</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#stub-functions">Stub functions</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#recursion1">Recursion</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#function-overloading">Function overloading</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#kernel-and-function-variables">Kernel and function variables</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#kernel/function-restrictions">Kernel/function restrictions</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#invoking-kernels">Invoking kernels</a>
   </li>
   </ul>
   </li>
   <li class="toctree-l2"><a class="reference internal" href="#profiling">Profiling</a>
   </li>
   <li class="toctree-l2"><a class="reference internal" href="#pointers">Pointers</a>
   <ul>
   <li class="toctree-l3"><a class="reference internal" href="#pointers-and-arrays">Pointers and Arrays</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#array-of-pointers">Array of pointers</a>
   </li>
   </ul>
   </li>
   <li class="toctree-l2"><a class="reference internal" href="#local-memory">Local memory</a>
   <ul>
   <li class="toctree-l3"><a class="reference internal" href="#local-buffers">Local Buffers</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#barriers">Barriers</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#local-work-size">Local Work Size</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#example-kernel-using-local-parameters">Example Kernel using local Parameters</a>
   </li>
   <li class="toctree-l3"><a class="reference internal" href="#example-kernel-using-local-variables">Example Kernel using local Variables</a>
   </li>
   </ul>
   </li>
   <li class="toctree-l2"><a class="reference internal" href="#explicit-memory-management">Explicit memory management</a>
   </li>
   <li class="toctree-l2"><a class="reference internal" href="#finishing">Finishing</a>
   </li>
   <li class="toctree-l2"><a class="reference internal" href="#double-precision">Double Precision</a>
   </li>
   <li class="toctree-l2"><a class="reference internal" href="#concurrent-use-of-gpus">Concurrent use of GPUs</a>
   </li>
   <li class="toctree-l2"><a class="reference internal" href="#notes">Notes</a>
   </li>
   </ul>
</li>
 <li class="toctree-l1"><a class="reference internal" href="distComp.html">Distributed Computing</a></li>
</ul><p class="caption"><span class="caption-text">Tools</span></p>
<ul>
 <li class="toctree-l1"><a class="reference internal" href="concc.html">Compiling Concurnas code</a></li>
 <li class="toctree-l1"><a class="reference internal" href="conc.html">Running Concurnas Programs</a></li>
 <li class="toctree-l1"><a class="reference internal" href="replIde.html">The Concurnas REPL</a></li>
</ul><p class="caption"><span class="caption-text">Others</span></p>
<ul>
 <li class="toctree-l1"><a class="reference internal" href="warn.html">Compiler Warnings</a></li>
 <li class="toctree-l1"><a class="reference internal" href="dsl.html">Domain Specific Languages</a></li>
 <li class="toctree-l1"><a class="reference internal" href="others.html">Other</a></li>
</ul>

                </div>
            </div>
        </nav>

            
        </section>

    <script type="text/javascript" src="../js/modernizr.min.js"></script>
    <script type="text/javascript" id="documentation_options" src="../js/documentation_options.js"></script>
    <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
    <script type="text/javascript" src="../js/underscore.js"></script>
    <script type="text/javascript" src="../js/doctools.js"></script>
    <script type="text/javascript" src="../js/client.min.js"></script>
    <script type="text/javascript" src="../js/doctheme.js"></script>
    <script type="text/javascript" src="../js/prism.js"></script>
    <script type="text/javascript" src="../js/cookies.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- add extra scripts here -->
    <!--<script async="async" src="https://cse.google.com/cse.js?cx=017863714325068112727:9mx2x5kwoyu"></script>-->

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-126537021-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-126537021-1');
    </script>

    <script type="text/javascript">
        jQuery(function() {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>


</body>

</html>


